{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPLiXB5DngJZcHW744tsjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EYBishal/Python-Training-Bootcamp/blob/main/15th-Jan-2026/Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j11EDl9_I0o_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('1STpROJECT').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "upload=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "XV9GMldUJfzQ",
        "outputId": "4056a7bf-8a93-487a-d23f-4266138a9324"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5dd785a1-631f-40f7-b676-bced1aeab5fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5dd785a1-631f-40f7-b676-bced1aeab5fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving orders.csv to orders.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/orders.csv'\n",
        "df=spark.read.csv(path,header=True,inferSchema=False)\n",
        "df.printSchema()\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcmcrSIrKfzC",
        "outputId": "849f0fe9-13a7-481d-b90b-83ee7fdc04e0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|\n",
            "|ORD00000005|    C000005|      Delhi|    Fashion|      Jeans|   8521|2024-01-06|Completed|\n",
            "|ORD00000006|    C000006|      Delhi|    Grocery|      Sugar|  42383|2024-01-07|Completed|\n",
            "|ORD00000007|    C000007|       Pune|    Grocery|       Rice|  45362|2024-01-08|Completed|\n",
            "|ORD00000008|    C000008|  Bangalore|    Fashion|      Jeans|  10563|2024-01-09|Completed|\n",
            "|ORD00000009|    C000009|    Kolkata|Electronics|     Laptop|  63715|2024-01-10|Completed|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvIipBIGLQXt",
        "outputId": "2c816aff-4063-4186-88f8-15f8d2327bc1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain why all columns must be treated as StringType initially ?**\n",
        "\n",
        "**Schema uncertainty:** Raw data often has inconsistent types; starting with strings avoids errors.\n",
        "\n",
        "**Preserve data:** Prevents loss of values that don't match inferred types.\n",
        "\n",
        "**Flexibility:** Easier to clean and validate before converting to correct types.\n",
        "\n",
        "**Avoid job failures:** Loading as strings reduces parsing issues in big data pipelines.\n",
        "\n",
        "**Custom transformations:** Gives full control for casting after data quality checks.\n"
      ],
      "metadata": {
        "id": "5fNWnpLELWcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "fix_col=['city','category','product']\n",
        "for i in fix_col:\n",
        "  df=df.withColumn(i,f.initcap(f.trim(i)))\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3wP4A5FMHqj",
        "outputId": "fc70fdc1-bcb0-48ed-e123-c2e19a588036"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n",
            "|   order_id|customer_id|     city|   category|    product| amount|order_date|   status|\n",
            "+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n",
            "|ORD00000000|    C000000|Hyderabad|    Grocery|        Oil|invalid|01/01/2024|Cancelled|\n",
            "|ORD00000001|    C000001|     Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|\n",
            "|ORD00000002|    C000002|     Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|\n",
            "|ORD00000003|    C000003|Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed|\n",
            "|ORD00000004|    C000004|     Pune|       Home|Airpurifier|  33659|2024-01-05|Completed|\n",
            "|ORD00000005|    C000005|    Delhi|    Fashion|      Jeans|   8521|2024-01-06|Completed|\n",
            "|ORD00000006|    C000006|    Delhi|    Grocery|      Sugar|  42383|2024-01-07|Completed|\n",
            "|ORD00000007|    C000007|     Pune|    Grocery|       Rice|  45362|2024-01-08|Completed|\n",
            "|ORD00000008|    C000008|Bangalore|    Fashion|      Jeans|  10563|2024-01-09|Completed|\n",
            "|ORD00000009|    C000009|  Kolkata|Electronics|     Laptop|  63715|2024-01-10|Completed|\n",
            "+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re_amount=f.regexp_extract(f.col(\"amount\"),r\"(\\d+)\",0)\n",
        "df=df.withColumn(\"amount\",f.when((re_amount==\"\")|re_amount.isNull(),f.lit(0)).otherwise(re_amount.cast(\"int\")))\n",
        "df.select('amount').show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oLdjf_UN0Vj",
        "outputId": "8b9f73db-b9ae-4ff2-dff5-0a5ad0c093bb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|amount|\n",
            "+------+\n",
            "|     0|\n",
            "| 35430|\n",
            "| 65358|\n",
            "|  5558|\n",
            "| 33659|\n",
            "|  8521|\n",
            "| 42383|\n",
            "| 45362|\n",
            "| 10563|\n",
            "| 63715|\n",
            "+------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df=df.withColumn(\"order_date\",f.trim(f.col(\"order_date\")))\n",
        "df=df.withColumn(\"order_date_clean\",f.coalesce(\n",
        "f.expr(\"try_to_timestamp(order_date, 'yyyy-MM-dd')\"),\n",
        "        f.expr(\"try_to_timestamp(order_date, 'dd/MM/yyyy')\"),\n",
        "        f.expr(\"try_to_timestamp(order_date, 'yyyy/MM/dd')\")\n",
        "    ))\n",
        "\n",
        "df.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "_Nbr7x0FQ8oO",
        "outputId": "f714c062-9497-49bb-a8b7-699e916e7297"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DateTimeException",
          "evalue": "[CANNOT_PARSE_TIMESTAMP] Text '12/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDateTimeException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1140701420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Check result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"order_date_clean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 )\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDateTimeException\u001b[0m: [CANNOT_PARSE_TIMESTAMP] Text '12/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_amount=df.filter(df.amount == 0).count()\n",
        "print(invalid_amount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwddUIScYG9y",
        "outputId": "1068ba9c-6e39-4070-f287-56ebf78a2bb6"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_date=df.filter(df.order_date_clean.isNull()).count()\n",
        "print(invalid_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKUEmV5Na-I0",
        "outputId": "d24ca82e-1a4f-425d-e3a9-8e8def8ef53d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dup_df=df.groupBy('order_id').count().filter(f.col('count')>1)\n",
        "dup_df.show()\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOYUX-Mac1S8",
        "outputId": "e533f29f-a667-4908-b07b-61ec82531b7c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|order_id|count|\n",
            "+--------+-----+\n",
            "+--------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates(['order_id'])\n",
        "df.count()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMB-8Q65et7d",
        "outputId": "681fd271-52b2-45ca-dd02-a202e83dc89e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.filter(f.col(\"status\")==\"Completed\")\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oaJ8PPlexkZ",
        "outputId": "15e755c7-3723-4a74-976c-b1f4992b673a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "285000"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_partitions = df.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions: {num_partitions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "349BlW46f9eN",
        "outputId": "7f4eafab-f28d-4d67-bab6-ed6d364f9687"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city_df=df.groupBy('city').agg(f.sum('amount').alias('total_amount'))\n",
        "city_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvaTwRqAi_Ab",
        "outputId": "1a4e7cc9-47b0-4622-ac39-4d0a5ebb5835"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|     city|total_amount|\n",
            "+---------+------------+\n",
            "|Bangalore|  1608818821|\n",
            "|  Chennai|  1609809323|\n",
            "|   Mumbai|  1605761872|\n",
            "|  Kolkata|  1604076741|\n",
            "|     Pune|  1626056695|\n",
            "|    Delhi|  1619020556|\n",
            "|Hyderabad|  1622219584|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city_df.explain(True)"
      ],
      "metadata": {
        "id": "mmCy22TnjLJj"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Identify where shuffle happens.**\n",
        "\n",
        "**Shuffle occurs at:**\n",
        "\n",
        "Exchange hashpartitioning(city, 200) before final aggregation.\n",
        "\n",
        "Exchange hashpartitioning(order_id, 200) during dedup/SortAggregate."
      ],
      "metadata": {
        "id": "_JpM9lURkHL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "repart_dp=df.repartition(\"city\")\n",
        ""
      ],
      "metadata": {
        "id": "vO5dcH3TjZGs"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Before repartition:\")\n",
        "df.explain(True)\n",
        "\n",
        "print(\"After repartition:\")\n",
        "repart_dp.explain(True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCngf0n-kcS0",
        "outputId": "334b87b6-8068-43b4-b7a3-678bfaab2bfc"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before repartition:\n",
            "== Parsed Logical Plan ==\n",
            "'Filter '`=`('status, Completed)\n",
            "+- Deduplicate [order_id#1419]\n",
            "   +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(try_to_timestamp(order_date#1515, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "      +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "         +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, order_date#1425, status#1426]\n",
            "            +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, initcap(trim(product#1423, None)) AS product#1475, amount#1424, order_date#1425, status#1426]\n",
            "               +- Project [order_id#1419, customer_id#1420, city#1473, initcap(trim(category#1422, None)) AS category#1474, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                  +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, category#1422, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                     +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: string, status: string, order_date_clean: timestamp\n",
            "Filter (status#1426 = Completed)\n",
            "+- Deduplicate [order_id#1419]\n",
            "   +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(try_to_timestamp(order_date#1515, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "      +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "         +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, order_date#1425, status#1426]\n",
            "            +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, initcap(trim(product#1423, None)) AS product#1475, amount#1424, order_date#1425, status#1426]\n",
            "               +- Project [order_id#1419, customer_id#1420, city#1473, initcap(trim(category#1422, None)) AS category#1474, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                  +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, category#1422, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                     +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Filter (isnotnull(status#1858) AND (status#1858 = Completed))\n",
            "+- Aggregate [order_id#1419], [order_id#1419, first(customer_id#1420, false) AS customer_id#1846, first(city#1473, false) AS city#1848, first(category#1474, false) AS category#1850, first(product#1475, false) AS product#1852, first(amount#1509, false) AS amount#1854, first(order_date#1515, false) AS order_date#1856, first(status#1426, false) AS status#1858, first(order_date_clean#1516, false) AS order_date_clean#1860]\n",
            "   +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(gettimestamp(order_date#1515, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "      +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, initcap(trim(category#1422, None)) AS category#1474, initcap(trim(product#1423, None)) AS product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "         +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Filter (isnotnull(status#1858) AND (status#1858 = Completed))\n",
            "   +- SortAggregate(key=[order_id#1419], functions=[first(customer_id#1420, false), first(city#1473, false), first(category#1474, false), first(product#1475, false), first(amount#1509, false), first(order_date#1515, false), first(status#1426, false), first(order_date_clean#1516, false)], output=[order_id#1419, customer_id#1846, city#1848, category#1850, product#1852, amount#1854, order_date#1856, status#1858, order_date_clean#1860])\n",
            "      +- Sort [order_id#1419 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(order_id#1419, 200), ENSURE_REQUIREMENTS, [plan_id=2616]\n",
            "            +- SortAggregate(key=[order_id#1419], functions=[partial_first(customer_id#1420, false), partial_first(city#1473, false), partial_first(category#1474, false), partial_first(product#1475, false), partial_first(amount#1509, false), partial_first(order_date#1515, false), partial_first(status#1426, false), partial_first(order_date_clean#1516, false)], output=[order_id#1419, first#1877, valueSet#1878, first#1879, valueSet#1880, first#1881, valueSet#1882, first#1883, valueSet#1884, first#1885, valueSet#1886, first#1887, valueSet#1888, first#1889, valueSet#1890, first#1891, valueSet#1892])\n",
            "               +- Sort [order_id#1419 ASC NULLS FIRST], false, 0\n",
            "                  +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(gettimestamp(order_date#1515, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "                     +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, initcap(trim(category#1422, None)) AS category#1474, initcap(trim(product#1423, None)) AS product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "                        +- FileScan csv [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n",
            "After repartition:\n",
            "== Parsed Logical Plan ==\n",
            "'RepartitionByExpression ['city]\n",
            "+- Filter (status#1426 = Completed)\n",
            "   +- Deduplicate [order_id#1419]\n",
            "      +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(try_to_timestamp(order_date#1515, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "         +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "            +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, order_date#1425, status#1426]\n",
            "               +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, initcap(trim(product#1423, None)) AS product#1475, amount#1424, order_date#1425, status#1426]\n",
            "                  +- Project [order_id#1419, customer_id#1420, city#1473, initcap(trim(category#1422, None)) AS category#1474, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                     +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, category#1422, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                        +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: string, status: string, order_date_clean: timestamp\n",
            "RepartitionByExpression [city#1473]\n",
            "+- Filter (status#1426 = Completed)\n",
            "   +- Deduplicate [order_id#1419]\n",
            "      +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(try_to_timestamp(order_date#1515, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "         +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "            +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, order_date#1425, status#1426]\n",
            "               +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, initcap(trim(product#1423, None)) AS product#1475, amount#1424, order_date#1425, status#1426]\n",
            "                  +- Project [order_id#1419, customer_id#1420, city#1473, initcap(trim(category#1422, None)) AS category#1474, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                     +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, category#1422, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "                        +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "RepartitionByExpression [city#1896]\n",
            "+- Filter (isnotnull(status#1906) AND (status#1906 = Completed))\n",
            "   +- Aggregate [order_id#1419], [order_id#1419, first(customer_id#1420, false) AS customer_id#1894, first(city#1473, false) AS city#1896, first(category#1474, false) AS category#1898, first(product#1475, false) AS product#1900, first(amount#1509, false) AS amount#1902, first(order_date#1515, false) AS order_date#1904, first(status#1426, false) AS status#1906, first(order_date_clean#1516, false) AS order_date_clean#1908]\n",
            "      +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(gettimestamp(order_date#1515, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "         +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, initcap(trim(category#1422, None)) AS category#1474, initcap(trim(product#1423, None)) AS product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "            +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Exchange hashpartitioning(city#1896, 200), REPARTITION_BY_COL, [plan_id=2652]\n",
            "   +- Filter (isnotnull(status#1906) AND (status#1906 = Completed))\n",
            "      +- SortAggregate(key=[order_id#1419], functions=[first(customer_id#1420, false), first(city#1473, false), first(category#1474, false), first(product#1475, false), first(amount#1509, false), first(order_date#1515, false), first(status#1426, false), first(order_date_clean#1516, false)], output=[order_id#1419, customer_id#1894, city#1896, category#1898, product#1900, amount#1902, order_date#1904, status#1906, order_date_clean#1908])\n",
            "         +- Sort [order_id#1419 ASC NULLS FIRST], false, 0\n",
            "            +- Exchange hashpartitioning(order_id#1419, 200), ENSURE_REQUIREMENTS, [plan_id=2648]\n",
            "               +- SortAggregate(key=[order_id#1419], functions=[partial_first(customer_id#1420, false), partial_first(city#1473, false), partial_first(category#1474, false), partial_first(product#1475, false), partial_first(amount#1509, false), partial_first(order_date#1515, false), partial_first(status#1426, false), partial_first(order_date_clean#1516, false)], output=[order_id#1419, first#1925, valueSet#1926, first#1927, valueSet#1928, first#1929, valueSet#1930, first#1931, valueSet#1932, first#1933, valueSet#1934, first#1935, valueSet#1936, first#1937, valueSet#1938, first#1939, valueSet#1940])\n",
            "                  +- Sort [order_id#1419 ASC NULLS FIRST], false, 0\n",
            "                     +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(gettimestamp(order_date#1515, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "                        +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, initcap(trim(category#1422, None)) AS category#1474, initcap(trim(product#1423, None)) AS product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "                           +- FileScan csv [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rev=df.groupBy('city').agg(f.sum('amount').alias('total_revenue'))\n",
        "df_rev.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZqqw9RiqrMG",
        "outputId": "3aa1d8d8-2d1d-4012-d92c-6d301b74aaca"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|Bangalore|   1608818821|\n",
            "|  Chennai|   1609809323|\n",
            "|   Mumbai|   1605761872|\n",
            "|  Kolkata|   1604076741|\n",
            "|     Pune|   1626056695|\n",
            "|    Delhi|   1619020556|\n",
            "|Hyderabad|   1622219584|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rev=df.groupBy('category').agg(f.sum('amount').alias('total_revenue'))\n",
        "df_rev.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYHbDQogqys7",
        "outputId": "4f2b9e0f-6d32-458f-95c2-42f1518c9513"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|       Home|   2833870208|\n",
            "|    Fashion|   2799225164|\n",
            "|    Grocery|   2831327086|\n",
            "|Electronics|   2831341134|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rev=df.groupBy('city').agg(f.avg('amount').alias('total_revenue'))\n",
        "df_rev.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y60yOR8Qq2OJ",
        "outputId": "b23d2ccd-4096-4f2b-9574-1565918957ed"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+\n",
            "|     city|     total_revenue|\n",
            "+---------+------------------+\n",
            "|Bangalore| 39910.16896132569|\n",
            "|  Chennai|39518.100034367635|\n",
            "|   Mumbai| 39539.09859154929|\n",
            "|  Kolkata| 39545.31817173286|\n",
            "|     Pune| 39773.41914732285|\n",
            "|    Delhi| 39629.42566211387|\n",
            "|Hyderabad| 39526.80451256061|\n",
            "+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rev=df.groupBy('product').agg(f.sum('amount').alias('total_revenue')).orderBy(f.desc('total_revenue')).limit(10)\n",
        "df_rev.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm_Z8D9Lq7qL",
        "outputId": "faf257f0-0ea6-4db4-ed7c-c9797e4a1644"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|    product|total_revenue|\n",
            "+-----------+-------------+\n",
            "|        Oil|    951812641|\n",
            "|     Laptop|    950556247|\n",
            "|     Tablet|    948911819|\n",
            "|     Vacuum|    948228359|\n",
            "|      Mixer|    945319858|\n",
            "|       Rice|    942865877|\n",
            "|Airpurifier|    940321991|\n",
            "|      Jeans|    939537887|\n",
            "|      Sugar|    936648568|\n",
            "|      Shoes|    935350562|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rev=df.groupBy('city').agg(f.sum('amount').alias('total_revenue')).orderBy(f.desc('total_revenue'))\n",
        "df_rev.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnhxROPXreP2",
        "outputId": "6d809c9f-e040-4714-cfd8-133d92ff959b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|     Pune|   1626056695|\n",
            "|Hyderabad|   1622219584|\n",
            "|    Delhi|   1619020556|\n",
            "|  Chennai|   1609809323|\n",
            "|Bangalore|   1608818821|\n",
            "|   Mumbai|   1605761872|\n",
            "|  Kolkata|   1604076741|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rank cities by revenue.\n",
        "2. Rank products inside each category by revenue.\n",
        "3. Find the top product for every category.\n",
        "4. Identify the top 3 performing cities."
      ],
      "metadata": {
        "id": "tq6s0_DJsyR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "s=df.groupBy('city').agg(f.sum('amount').alias('total_amount'))\n",
        "w = Window.orderBy(f.desc(\"total_amount\"))\n",
        "df_rev=s.withColumn(\"rank\",f.rank().over(w))\n",
        "df_rev.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxx2OGTgrqEP",
        "outputId": "0ecb0791-36d3-409a-e21e-4d61ca5c80e8"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+----+\n",
            "|     city|total_amount|rank|\n",
            "+---------+------------+----+\n",
            "|     Pune|  1626056695|   1|\n",
            "|Hyderabad|  1622219584|   2|\n",
            "|    Delhi|  1619020556|   3|\n",
            "|  Chennai|  1609809323|   4|\n",
            "|Bangalore|  1608818821|   5|\n",
            "|   Mumbai|  1605761872|   6|\n",
            "|  Kolkata|  1604076741|   7|\n",
            "+---------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "product_rev = df.groupBy(\"category\", \"product\").agg(f.sum(\"amount\").alias(\"total_revenue\"))\n",
        "w = Window.partitionBy(\"category\").orderBy(f.desc(\"total_revenue\"))\n",
        "\n",
        "ranked_products = product_rev.withColumn(\"rank\", f.rank().over(w))\n",
        "ranked_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwS0Sf0vve4G",
        "outputId": "94c46d22-da7f-45f5-d5f1-bfd04dce682e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-------------+----+\n",
            "|   category|    product|total_revenue|rank|\n",
            "+-----------+-----------+-------------+----+\n",
            "|Electronics|     Laptop|    950556247|   1|\n",
            "|Electronics|     Tablet|    948911819|   2|\n",
            "|Electronics|     Mobile|    931873068|   3|\n",
            "|    Fashion|      Jeans|    939537887|   1|\n",
            "|    Fashion|      Shoes|    935350562|   2|\n",
            "|    Fashion|     Tshirt|    924336715|   3|\n",
            "|    Grocery|        Oil|    951812641|   1|\n",
            "|    Grocery|       Rice|    942865877|   2|\n",
            "|    Grocery|      Sugar|    936648568|   3|\n",
            "|       Home|     Vacuum|    948228359|   1|\n",
            "|       Home|      Mixer|    945319858|   2|\n",
            "|       Home|Airpurifier|    940321991|   3|\n",
            "+-----------+-----------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product_rev = df.groupBy(\"category\", \"product\").agg(f.sum(\"amount\").alias(\"total_revenue\"))\n",
        "w = Window.partitionBy(\"category\").orderBy(f.desc(\"total_revenue\"))\n",
        "\n",
        "ranked_products = product_rev.withColumn(\"rank\", f.rank().over(w)).filter(f.col(\"rank\")==1)\n",
        "ranked_products.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq4SJonixzUJ",
        "outputId": "5def22c5-b89a-456a-a17c-be55dd910ba2"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------------+----+\n",
            "|   category|product|total_revenue|rank|\n",
            "+-----------+-------+-------------+----+\n",
            "|Electronics| Laptop|    950556247|   1|\n",
            "|    Fashion|  Jeans|    939537887|   1|\n",
            "|    Grocery|    Oil|    951812641|   1|\n",
            "|       Home| Vacuum|    948228359|   1|\n",
            "+-----------+-------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product_rev=df.groupBy(\"city\").agg(f.sum(\"amount\").alias(\"total_revenue\"))\n",
        "w=Window.orderBy(f.desc(\"total_revenue\"))\n",
        "\n",
        "ranked_products = product_rev.withColumn(\"rank\", f.rank().over(w)).filter(f.col(\"rank\")<=3)\n",
        "ranked_products.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjRWvYaRydok",
        "outputId": "7dca1186-4096-4a14-cda7-dbfcac396d99"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+----+\n",
            "|     city|total_revenue|rank|\n",
            "+---------+-------------+----+\n",
            "|     Pune|   1626056695|   1|\n",
            "|Hyderabad|   1622219584|   2|\n",
            "|    Delhi|   1619020556|   3|\n",
            "+---------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "city_region_data = [\n",
        "    Row(city=\"Delhi\", region=\"North\"),\n",
        "    Row(city=\"Mumbai\", region=\"West\"),\n",
        "    Row(city=\"Bangalore\", region=\"South\"),\n",
        "    Row(city=\"Hyderabad\", region=\"South\"),\n",
        "    Row(city=\"Pune\", region=\"West\"),\n",
        "    Row(city=\"Chennai\", region=\"South\"),\n",
        "    Row(city=\"Kolkata\", region=\"East\")\n",
        "]\n",
        "\n",
        "city_region_df = spark.createDataFrame(city_region_data)\n",
        "city_region_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voyWgpUOzBuq",
        "outputId": "1382e2c3-65b6-4002-bbb8-55b36389956f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|    Delhi| North|\n",
            "|   Mumbai|  West|\n",
            "|Bangalore| South|\n",
            "|Hyderabad| South|\n",
            "|     Pune|  West|\n",
            "|  Chennai| South|\n",
            "|  Kolkata|  East|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "joined_df = df.join(broadcast(city_region_df), on=\"city\", how=\"inner\")\n"
      ],
      "metadata": {
        "id": "b0qdIGLu1Wc0"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8gTztSt1cJa",
        "outputId": "927584b9-293a-43a2-e9c9-8e4ded3d4340"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (status#1426 = Completed)\n",
            ":  +- Deduplicate [order_id#1419]\n",
            ":     +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(try_to_timestamp(order_date#1515, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            ":        +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            ":           +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, order_date#1425, status#1426]\n",
            ":              +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, initcap(trim(product#1423, None)) AS product#1475, amount#1424, order_date#1425, status#1426]\n",
            ":                 +- Project [order_id#1419, customer_id#1420, city#1473, initcap(trim(category#1422, None)) AS category#1474, product#1423, amount#1424, order_date#1425, status#1426]\n",
            ":                    +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, category#1422, product#1423, amount#1424, order_date#1425, status#1426]\n",
            ":                       +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#3755, region#3756], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, order_id: string, customer_id: string, category: string, product: string, amount: int, order_date: string, status: string, order_date_clean: timestamp, region: string\n",
            "Project [city#1473, order_id#1419, customer_id#1420, category#1474, product#1475, amount#1509, order_date#1515, status#1426, order_date_clean#1516, region#3756]\n",
            "+- Join Inner, (city#1473 = city#3755)\n",
            "   :- Filter (status#1426 = Completed)\n",
            "   :  +- Deduplicate [order_id#1419]\n",
            "   :     +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(try_to_timestamp(order_date#1515, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#1515, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "   :        +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "   :           +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, order_date#1425, status#1426]\n",
            "   :              +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, initcap(trim(product#1423, None)) AS product#1475, amount#1424, order_date#1425, status#1426]\n",
            "   :                 +- Project [order_id#1419, customer_id#1420, city#1473, initcap(trim(category#1422, None)) AS category#1474, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "   :                    +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, category#1422, product#1423, amount#1424, order_date#1425, status#1426]\n",
            "   :                       +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#3755, region#3756], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#3767, order_id#1419, customer_id#3765, category#3769, product#3771, amount#3773, order_date#3775, status#3777, order_date_clean#3779, region#3756]\n",
            "+- Join Inner, (city#3767 = city#3755), rightHint=(strategy=broadcast)\n",
            "   :- Filter ((isnotnull(status#3777) AND (status#3777 = Completed)) AND isnotnull(city#3767))\n",
            "   :  +- Aggregate [order_id#1419], [order_id#1419, first(customer_id#1420, false) AS customer_id#3765, first(city#1473, false) AS city#3767, first(category#1474, false) AS category#3769, first(product#1475, false) AS product#3771, first(amount#1509, false) AS amount#3773, first(order_date#1515, false) AS order_date#3775, first(status#1426, false) AS status#3777, first(order_date_clean#1516, false) AS order_date_clean#3779]\n",
            "   :     +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(gettimestamp(order_date#1515, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "   :        +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, initcap(trim(category#1422, None)) AS category#1474, initcap(trim(product#1423, None)) AS product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "   :           +- Relation [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] csv\n",
            "   +- Filter isnotnull(city#3755)\n",
            "      +- LogicalRDD [city#3755, region#3756], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#3767, order_id#1419, customer_id#3765, category#3769, product#3771, amount#3773, order_date#3775, status#3777, order_date_clean#3779, region#3756]\n",
            "   +- BroadcastHashJoin [city#3767], [city#3755], Inner, BuildRight, false\n",
            "      :- Filter ((isnotnull(status#3777) AND (status#3777 = Completed)) AND isnotnull(city#3767))\n",
            "      :  +- SortAggregate(key=[order_id#1419], functions=[first(customer_id#1420, false), first(city#1473, false), first(category#1474, false), first(product#1475, false), first(amount#1509, false), first(order_date#1515, false), first(status#1426, false), first(order_date_clean#1516, false)], output=[order_id#1419, customer_id#3765, city#3767, category#3769, product#3771, amount#3773, order_date#3775, status#3777, order_date_clean#3779])\n",
            "      :     +- Sort [order_id#1419 ASC NULLS FIRST], false, 0\n",
            "      :        +- Exchange hashpartitioning(order_id#1419, 200), ENSURE_REQUIREMENTS, [plan_id=6232]\n",
            "      :           +- SortAggregate(key=[order_id#1419], functions=[partial_first(customer_id#1420, false), partial_first(city#1473, false), partial_first(category#1474, false), partial_first(product#1475, false), partial_first(amount#1509, false), partial_first(order_date#1515, false), partial_first(status#1426, false), partial_first(order_date_clean#1516, false)], output=[order_id#1419, first#3796, valueSet#3797, first#3798, valueSet#3799, first#3800, valueSet#3801, first#3802, valueSet#3803, first#3804, valueSet#3805, first#3806, valueSet#3807, first#3808, valueSet#3809, first#3810, valueSet#3811])\n",
            "      :              +- Sort [order_id#1419 ASC NULLS FIRST], false, 0\n",
            "      :                 +- Project [order_id#1419, customer_id#1420, city#1473, category#1474, product#1475, amount#1509, order_date#1515, status#1426, coalesce(gettimestamp(order_date#1515, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#1515, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS order_date_clean#1516]\n",
            "      :                    +- Project [order_id#1419, customer_id#1420, initcap(trim(city#1421, None)) AS city#1473, initcap(trim(category#1422, None)) AS category#1474, initcap(trim(product#1423, None)) AS product#1475, CASE WHEN ((regexp_extract(amount#1424, (\\d+), 0) = ) OR isnull(regexp_extract(amount#1424, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#1424, (\\d+), 0) as int) END AS amount#1509, trim(order_date#1425, None) AS order_date#1515, status#1426]\n",
            "      :                       +- FileScan csv [order_id#1419,customer_id#1420,city#1421,category#1422,product#1423,amount#1424,order_date#1425,status#1426] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=6237]\n",
            "         +- Filter isnotnull(city#3755)\n",
            "            +- Scan ExistingRDD[city#3755,region#3756]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Explain why broadcast join is efficient in this case.**\n",
        "\n",
        "The city-region dataset is very small (7 rows).\n",
        "\n",
        "Spark broadcasts it to all executors  avoids shuffle  faster join."
      ],
      "metadata": {
        "id": "XxwOEHQi3EHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "def amount_classify(amount):\n",
        "  if amount >= 80000:\n",
        "    return \"High\"\n",
        "  elif amount >= 40000:\n",
        "    return \"Medium\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "classify_udf=f.udf(amount_classify,StringType())\n",
        "df = df.withColumn(\"order_value_category\", classify_udf(f.col(\"amount\")))\n",
        "df.groupBy(\"order_value_category\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxnwVYZT3OKs",
        "outputId": "6dc5875a-1708-4877-fefb-dc7308a5cf43"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|order_value_category| count|\n",
            "+--------------------+------+\n",
            "|                High| 27936|\n",
            "|                 Low|145699|\n",
            "|              Medium|111365|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rdd = df.rdd\n",
        "print(rdd.take(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWwIKRqa5I9n",
        "outputId": "efdd0563-354e-4fd8-c5cb-7c8427f42a62"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(order_id='ORD00000001', customer_id='C000001', city='Pune', category='Grocery', product='Sugar', amount=35430, order_date='2024-01-02', status='Completed', order_date_clean=datetime.datetime(2024, 1, 2, 0, 0), order_value_category='Low'), Row(order_id='ORD00000007', customer_id='C000007', city='Pune', category='Grocery', product='Rice', amount=45362, order_date='2024-01-08', status='Completed', order_date_clean=datetime.datetime(2024, 1, 8, 0, 0), order_value_category='Medium'), Row(order_id='ORD00000008', customer_id='C000008', city='Bangalore', category='Fashion', product='Jeans', amount=10563, order_date='2024-01-09', status='Completed', order_date_clean=datetime.datetime(2024, 1, 9, 0, 0), order_value_category='Low'), Row(order_id='ORD00000010', customer_id='C000010', city='Bangalore', category='Grocery', product='Sugar', amount=66576, order_date='2024-01-11', status='Completed', order_date_clean=datetime.datetime(2024, 1, 11, 0, 0), order_value_category='Medium'), Row(order_id='ORD00000011', customer_id='C000011', city='Kolkata', category='Electronics', product='Tablet', amount=50318, order_date='12/01/2024', status='Completed', order_date_clean=datetime.datetime(2024, 1, 12, 0, 0), order_value_category='Medium')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saly=df.select('amount').rdd.map(lambda x:x[0])\n",
        "tot=saly.reduce(lambda x,y:x+y)\n",
        "print(tot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmfpmf-j6qo8",
        "outputId": "03e1ed2b-f74b-4f20-d49a-03baefbd1fc6"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11295763592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "orders_per_city = (\n",
        "    df.select(\"city\").rdd\n",
        "      .map(lambda r: (r[0], 1))\n",
        "      .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "print(orders_per_city.take(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeG3I6rY8tbv",
        "outputId": "75dce51f-cc35-4f3b-c835-b988c149563a"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Pune', 40883), ('Mumbai', 40612), ('Hyderabad', 41041), ('Delhi', 40854), ('Bangalore', 40311), ('Kolkata', 40563), ('Chennai', 40736)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why DataFrames are preferred over RDDs for analytics (short)**\n",
        "\n",
        "**Columnar memory + off-heap execution:** Better CPU cache usage, compression, and vectorized processing  lower memory and CPU cost for scans/aggregations.\n",
        "\n",
        "**Declarative, concise, and safer:** High-level APIs (select, groupBy, agg) reduce boilerplate and bugs, and integrate seamlessly with Spark SQL."
      ],
      "metadata": {
        "id": "TK8haG9L---K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identify datasets reused in multiple queries.**\n",
        "\n",
        "orders_df = df.filter(F.col(\"status\") == \"Completed\")"
      ],
      "metadata": {
        "id": "nYyxAMNm_iVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PrzKNkg_ynH",
        "outputId": "160fc126-9379-4c6d-b0e4-db5e30b39809"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: string, status: string, order_date_clean: timestamp, order_value_category: string]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsuunW2e_0fO",
        "outputId": "27b733a4-ade8-4d95-9f37-e2fadb9ee3f4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: string, status: string, order_date_clean: timestamp, order_value_category: string]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "orders_df = df.filter(f.col(\"status\") == \"Completed\")\n",
        "\n",
        "# ---------------- BEFORE CACHING ----------------\n",
        "start = time.time()\n",
        "\n",
        "orders_df.agg(f.sum(\"amount\").alias(\"total_revenue\"))\n",
        "\n",
        "orders_df.groupBy(\"city\").count()\n",
        "\n",
        "orders_df.groupBy(\"category\").agg(f.sum(\"amount\").alias(\"category_revenue\"))\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Time before caching: {end - start:.2f} seconds\")\n",
        "\n",
        "# ---------------- APPLY CACHE ----------------\n",
        "orders_df.cache()\n",
        "\n",
        "# ---------------- AFTER CACHING ----------------\n",
        "start = time.time()\n",
        "\n",
        "orders_df.agg(f.sum(\"amount\").alias(\"total_revenue\"))\n",
        "orders_df.groupBy(\"city\").count()\n",
        "orders_df.groupBy(\"category\").agg(f.sum(\"amount\").alias(\"category_revenue\"))\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Time after caching: {end - start:.2f} seconds\")\n",
        "\n",
        "# ---------------- UNPERSIST ----------------\n",
        "orders_df.unpersist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK5axEitAJLy",
        "outputId": "e187276d-5d34-4072-bb1c-cfa37e9a5782"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time before caching: 0.05 seconds\n",
            "Time after caching: 0.03 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: string, status: string, order_date_clean: timestamp, order_value_category: string]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain why unnecessary caching is dangerous.**\n",
        "\n",
        "**Consumes memory:** Cached data stays in RAM, reducing space for other tasks.\n",
        "\n",
        "**May cause spills:** If memory is full, Spark spills to disk  slows down jobs.\n",
        "\n",
        "**Wasted resources:** If cached data is never reused, caching adds overhead without benefit."
      ],
      "metadata": {
        "id": "_jpCRlAKA0HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/path/cleaned_orders_parquet\")"
      ],
      "metadata": {
        "id": "mBD7HYkkBK-9"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agg_df = df.groupBy(\"city\").agg(f.sum(\"amount\").alias(\"total_revenue\"))\n",
        "agg_df.write.mode(\"overwrite\").orc(\"/path/aggregated_orders_orc\")\n"
      ],
      "metadata": {
        "id": "uHz6A_unBPyO"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parquet_df = spark.read.parquet(\"/path/cleaned_orders_parquet\")\n",
        "orc_df = spark.read.orc(\"/path/aggregated_orders_orc\")\n",
        "\n",
        "parquet_df.printSchema()\n",
        "orc_df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DWpvquTBU03",
        "outputId": "bf284869-414c-4786-a509-9ceb7807b208"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- order_date_clean: timestamp (nullable = true)\n",
            " |-- order_value_category: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- city: string (nullable = true)\n",
            " |-- total_revenue: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain why this breaks:**\n",
        "\n",
        "**df = df.filter(df.amount > 50000).show()**\n",
        "**And why after this line df is no longer a DataFrame.**\n",
        "\n",
        ".show() returns None, not a DataFrame.\n",
        " df is None, so df is no longer a DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "0HIFxjlRB0QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "\n",
        "critical_fields = [\"order_id\", \"amount\", \"order_date_clean\"]\n",
        "for col in critical_fields:\n",
        "    print(f\"{col} null count:\", df.filter(f.col(col).isNull()).count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXnymrxeBcP4",
        "outputId": "6a3fa700-a7c5-4e0d-932f-4102111166ef"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- order_date_clean: timestamp (nullable = true)\n",
            " |-- order_value_category: string (nullable = true)\n",
            "\n",
            "order_id null count: 0\n",
            "amount null count: 0\n",
            "order_date_clean null count: 2465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document:**\n",
        "\n",
        "**Cleaning strategy:**\n",
        "\n",
        "Normalized dates, handled invalid placeholders, cast amount to integer, removed nulls.\n",
        "\n",
        "\n",
        "**Performance strategy:**\n",
        "\n",
        "Used cache() for reused datasets, partitioned writes, broadcast joins for small lookup tables.\n",
        "\n",
        "\n",
        "**Debugging learnings:**\n",
        "\n",
        "Avoid chaining .show() when assigning DataFrames.\n",
        "Validate schema after transformations.\n",
        "Use Spark UI for performance tuning."
      ],
      "metadata": {
        "id": "Vfs666ujBpHR"
      }
    }
  ]
}